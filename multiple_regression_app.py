import streamlit as st
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score

# è¨­å®šé é¢é…ç½®
st.set_page_config(
    page_title="CRISP-DM å¤šå…ƒè¿´æ­¸ç¯„ä¾‹", 
    layout="wide",
    initial_sidebar_state="expanded"
)

# æ¨™é¡Œ
st.title("ğŸ”¬ å¤šå…ƒè¿´æ­¸äº’å‹•ç¯„ä¾‹")
st.markdown("**Multiple Linear Regression: y = axâ‚ + bxâ‚‚ + c + noise**")
st.markdown("---")

# å´é‚Šæ¬„ - åƒæ•¸è¨­å®š
st.sidebar.header("ğŸ“Š è³‡æ–™åƒæ•¸è¨­å®š")

# 1. è³‡æ–™ç”Ÿæˆåƒæ•¸
n_points = st.sidebar.slider(
    "è³‡æ–™é»æ•¸é‡ (n)", 
    min_value=100, 
    max_value=1000, 
    value=500, 
    step=50,
    help="ç”Ÿæˆå¤šå°‘å€‹è³‡æ–™é»"
)

# å¤šå…ƒè¿´æ­¸ä¿‚æ•¸
coefficient_a = st.sidebar.slider(
    "ä¿‚æ•¸ a (xâ‚)", 
    min_value=-10.0, 
    max_value=10.0, 
    value=2.0, 
    step=0.1,
    help="y = axâ‚ + bxâ‚‚ + c + noise ä¸­çš„ä¿‚æ•¸ a"
)

coefficient_b = st.sidebar.slider(
    "ä¿‚æ•¸ b (xâ‚‚)", 
    min_value=-10.0, 
    max_value=10.0, 
    value=1.5, 
    step=0.1,
    help="y = axâ‚ + bxâ‚‚ + c + noise ä¸­çš„ä¿‚æ•¸ b"
)

intercept_c = st.sidebar.slider(
    "æˆªè· c", 
    min_value=-20.0, 
    max_value=20.0, 
    value=5.0, 
    step=0.5,
    help="y = axâ‚ + bxâ‚‚ + c + noise ä¸­çš„æˆªè· c"
)

noise_variance = st.sidebar.slider(
    "é›œè¨Šè®Šç•°æ•¸ (var)", 
    min_value=0, 
    max_value=1000, 
    value=100, 
    step=10,
    help="é›œè¨Šçš„è®Šç•°æ•¸ï¼Œå½±éŸ¿è³‡æ–™çš„åˆ†æ•£ç¨‹åº¦"
)

# è¨­å®šéš¨æ©Ÿç¨®å­ä»¥ç¢ºä¿çµæœå¯é‡ç¾
random_seed = st.sidebar.number_input(
    "éš¨æ©Ÿç¨®å­", 
    min_value=0, 
    max_value=9999, 
    value=42,
    help="è¨­å®šéš¨æ©Ÿç¨®å­ä»¥ç¢ºä¿çµæœå¯é‡ç¾"
)

# CRISP-DM éšæ®µæ¨™ç¤º
st.sidebar.markdown("---")
st.sidebar.markdown("### ğŸ”„ CRISP-DM éšæ®µ")
st.sidebar.markdown("âœ… 1. å•†æ¥­ç†è§£")
st.sidebar.markdown("âœ… 2. è³‡æ–™ç†è§£") 
st.sidebar.markdown("âœ… 3. è³‡æ–™æº–å‚™")
st.sidebar.markdown("âœ… 4. å»ºæ¨¡")
st.sidebar.markdown("âœ… 5. è©•ä¼°")
st.sidebar.markdown("âœ… 6. éƒ¨ç½²")

# ä¸»è¦å…§å®¹å€åŸŸ
col1, col2 = st.columns([2, 1])

with col1:
    st.subheader("ğŸ“ˆ å¤šå…ƒè¿´æ­¸è¦–è¦ºåŒ–")
    
    # ç”Ÿæˆè³‡æ–™ (CRISP-DM 3.0 è³‡æ–™æº–å‚™)
    np.random.seed(random_seed)
    x1 = np.random.rand(n_points) * 20 - 10  # x1ç¯„åœï¼š-10åˆ°10
    x2 = np.random.rand(n_points) * 20 - 10  # x2ç¯„åœï¼š-10åˆ°10
    y_true = coefficient_a * x1 + coefficient_b * x2 + intercept_c
    noise = np.random.normal(0, np.sqrt(noise_variance), n_points)
    y = y_true + noise
    
    # å»ºç«‹DataFrame
    df = pd.DataFrame({'x1': x1, 'x2': x2, 'y': y})
    
    # åŸ·è¡Œå¤šå…ƒè¿´æ­¸ (CRISP-DM 4.0 å»ºæ¨¡)
    model = LinearRegression()
    X = df[['x1', 'x2']]
    model.fit(X, y)
    y_pred = model.predict(X)
    
    # è¨ˆç®—æ®˜å·®ç”¨æ–¼é›¢ç¾¤å€¼æª¢æ¸¬ (CRISP-DM 5.0 è©•ä¼°)
    residuals = np.abs(y - y_pred)
    df['residuals'] = residuals
    
    # è­˜åˆ¥å‰5å€‹é›¢ç¾¤å€¼
    outliers = df.nlargest(5, 'residuals')
    
    # å»ºç«‹3Dåœ–è¡¨
    fig = plt.figure(figsize=(12, 8))
    ax = fig.add_subplot(111, projection='3d')
    
    # ç¹ªè£½è³‡æ–™é»
    ax.scatter(df['x1'], df['x2'], df['y'], 
               alpha=0.6, 
               s=30, 
               color='blue', 
               label=f'Data Points (n={n_points})')
    
    # å»ºç«‹è¿´æ­¸å¹³é¢ç¶²æ ¼
    x1_range = np.linspace(df['x1'].min(), df['x1'].max(), 20)
    x2_range = np.linspace(df['x2'].min(), df['x2'].max(), 20)
    X1_grid, X2_grid = np.meshgrid(x1_range, x2_range)
    Y_grid = model.coef_[0] * X1_grid + model.coef_[1] * X2_grid + model.intercept_
    
    # ç¹ªè£½è¿´æ­¸å¹³é¢
    ax.plot_surface(X1_grid, X2_grid, Y_grid, 
                    alpha=0.3, 
                    color='red', 
                    label='Regression Plane')
    
    # æ¨™è¨˜é›¢ç¾¤å€¼
    for i, (idx, row) in enumerate(outliers.iterrows()):
        ax.scatter(row['x1'], row['x2'], row['y'], 
                  color='purple', 
                  s=200, 
                  edgecolors='black', 
                  linewidth=2,
                  zorder=5)
        ax.text(row['x1'], row['x2'], row['y'], 
               f'Outlier {i+1}', 
               fontsize=8, 
               color='purple',
               fontweight='bold')
    
    ax.set_xlabel("Xâ‚ Values", fontsize=12)
    ax.set_ylabel("Xâ‚‚ Values", fontsize=12)
    ax.set_zlabel("Y Values", fontsize=12)
    ax.set_title(f"Multiple Linear Regression (y = {coefficient_a:.1f}xâ‚ + {coefficient_b:.1f}xâ‚‚ + {intercept_c:.1f} + noise)", 
                fontsize=14, fontweight='bold')
    
    st.pyplot(fig)
    
    # 2DæŠ•å½±åœ–è¡¨
    st.subheader("ğŸ“Š 2D æŠ•å½±è¦–åœ–")
    
    # é¸æ“‡æŠ•å½±è»¸
    projection_axis = st.selectbox("é¸æ“‡æŠ•å½±è»¸", ["Xâ‚-Y", "Xâ‚‚-Y"])
    
    fig2, ax2 = plt.subplots(figsize=(10, 6))
    
    if projection_axis == "Xâ‚-Y":
        ax2.scatter(df['x1'], df['y'], alpha=0.6, s=30, color='blue', label='Data Points')
        
        # ç¹ªè£½æŠ•å½±è¿´æ­¸ç·š
        x1_sorted = np.sort(df['x1'])
        y_proj = model.coef_[0] * x1_sorted + model.coef_[1] * df['x2'].mean() + model.intercept_
        ax2.plot(x1_sorted, y_proj, color='red', linewidth=3, label='Projected Regression Line')
        
        # æ¨™è¨˜é›¢ç¾¤å€¼
        for i, (idx, row) in enumerate(outliers.iterrows()):
            ax2.scatter(row['x1'], row['y'], color='purple', s=150, edgecolors='black', linewidth=2, zorder=5)
            ax2.annotate(f'Outlier {i+1}', (row['x1'], row['y']), 
                        textcoords="offset points", xytext=(0, 15), ha='center', color='purple', fontweight='bold')
        
        ax2.set_xlabel("Xâ‚ Values", fontsize=12)
        ax2.set_ylabel("Y Values", fontsize=12)
        ax2.set_title("Xâ‚-Y Projection", fontsize=14, fontweight='bold')
        
    else:  # Xâ‚‚-Y
        ax2.scatter(df['x2'], df['y'], alpha=0.6, s=30, color='blue', label='Data Points')
        
        # ç¹ªè£½æŠ•å½±è¿´æ­¸ç·š
        x2_sorted = np.sort(df['x2'])
        y_proj = model.coef_[0] * df['x1'].mean() + model.coef_[1] * x2_sorted + model.intercept_
        ax2.plot(x2_sorted, y_proj, color='red', linewidth=3, label='Projected Regression Line')
        
        # æ¨™è¨˜é›¢ç¾¤å€¼
        for i, (idx, row) in enumerate(outliers.iterrows()):
            ax2.scatter(row['x2'], row['y'], color='purple', s=150, edgecolors='black', linewidth=2, zorder=5)
            ax2.annotate(f'Outlier {i+1}', (row['x2'], row['y']), 
                        textcoords="offset points", xytext=(0, 15), ha='center', color='purple', fontweight='bold')
        
        ax2.set_xlabel("Xâ‚‚ Values", fontsize=12)
        ax2.set_ylabel("Y Values", fontsize=12)
        ax2.set_title("Xâ‚‚-Y Projection", fontsize=14, fontweight='bold')
    
    ax2.legend(fontsize=10)
    ax2.grid(True, alpha=0.3)
    st.pyplot(fig2)

with col2:
    st.subheader("ğŸ“Š æ¨¡å‹çµæœ")
    
    # é¡¯ç¤ºæ¨¡å‹ä¿‚æ•¸
    st.markdown("### ğŸ¯ è¿´æ­¸ä¿‚æ•¸")
    st.metric("ä¿‚æ•¸ a (xâ‚)", f"{model.coef_[0]:.3f}")
    st.metric("ä¿‚æ•¸ b (xâ‚‚)", f"{model.coef_[1]:.3f}")
    st.metric("æˆªè· c", f"{model.intercept_:.3f}")
    
    # è¨ˆç®—RÂ²åˆ†æ•¸
    r2 = r2_score(y, y_pred)
    st.metric("RÂ² åˆ†æ•¸", f"{r2:.3f}")
    
    # è¨ˆç®—èª¿æ•´å¾ŒRÂ²
    n = len(y)
    p = 2  # ç‰¹å¾µæ•¸é‡
    adj_r2 = 1 - (1 - r2) * (n - 1) / (n - p - 1)
    st.metric("èª¿æ•´å¾Œ RÂ²", f"{adj_r2:.3f}")
    
    # é¡¯ç¤ºé›¢ç¾¤å€¼è³‡è¨Š
    st.markdown("### ğŸ” å‰5å€‹é›¢ç¾¤å€¼")
    outliers_display = outliers[['x1', 'x2', 'y', 'residuals']].copy()
    outliers_display.columns = ['Xâ‚å€¼', 'Xâ‚‚å€¼', 'Yå€¼', 'æ®˜å·®']
    outliers_display.index = [f'é›¢ç¾¤å€¼ {i+1}' for i in range(len(outliers_display))]
    st.dataframe(outliers_display.round(3), use_container_width=True)
    
    # é¡¯ç¤ºè³‡æ–™çµ±è¨ˆ
    st.markdown("### ğŸ“ˆ è³‡æ–™çµ±è¨ˆ")
    st.write(f"è³‡æ–™é»ç¸½æ•¸: {n_points}")
    st.write(f"Xâ‚ ç¯„åœ: [{df['x1'].min():.2f}, {df['x1'].max():.2f}]")
    st.write(f"Xâ‚‚ ç¯„åœ: [{df['x2'].min():.2f}, {df['x2'].max():.2f}]")
    st.write(f"Y ç¯„åœ: [{df['y'].min():.2f}, {df['y'].max():.2f}]")

# åº•éƒ¨è³‡è¨Š
st.markdown("---")
st.markdown("### ğŸ“‹ å¤šå…ƒè¿´æ­¸èªªæ˜")
st.markdown("""
**å¤šå…ƒç·šæ€§è¿´æ­¸ (Multiple Linear Regression)** æ˜¯ç°¡å–®ç·šæ€§è¿´æ­¸çš„å»¶ä¼¸ï¼š

- **æ¨¡å‹å…¬å¼**: y = axâ‚ + bxâ‚‚ + c + noise
- **ç‰¹å¾µæ•¸é‡**: 2å€‹ç¨ç«‹è®Šæ•¸ (xâ‚, xâ‚‚)
- **è¦–è¦ºåŒ–**: 3Dæ•£é»åœ– + è¿´æ­¸å¹³é¢
- **è©•ä¼°æŒ‡æ¨™**: RÂ²ã€èª¿æ•´å¾ŒRÂ²ã€æ®˜å·®åˆ†æ

**èˆ‡ç°¡å–®ç·šæ€§è¿´æ­¸çš„å·®ç•°**ï¼š
- ç°¡å–®è¿´æ­¸: y = ax + b + noise (1å€‹ç‰¹å¾µ)
- å¤šå…ƒè¿´æ­¸: y = axâ‚ + bxâ‚‚ + c + noise (2å€‹ç‰¹å¾µ)

æœ¬ç¯„ä¾‹å±•ç¤ºäº†å®Œæ•´çš„CRISP-DMæµç¨‹åœ¨å¤šå…ƒè¿´æ­¸ä¸­çš„æ‡‰ç”¨ã€‚
""")
